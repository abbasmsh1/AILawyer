{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = 'daacc8dc45f272f48e8571c2ff9bbccc7169541e632faa75e7efa12900cf2813'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This document is a project proposal for an inspection assistance system, specifically designed for the manufacturing sector in the automotive industry, with a focus on inspecting Electric Vehicle (EV) batteries. The core concept involves utilizing a wearable device, such as smart glasses, to capture live video feeds of the battery from multiple angles. The system will then overlay any detected defects or anomalies onto the user's view in real-time, enhancing the efficiency of the inspection process. The document delves into various components of the project, including the system's architecture, software requirements, features, functional specifications, and implementation strategies.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.embeddings.together import TogetherEmbedding\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Provide a template following the LLM's original chat template.\n",
    "def completion_to_prompt(completion: str) -> str:\n",
    "  return f\"<s>[INST] {completion} [/INST] </s>\\n\"\n",
    "\n",
    "\n",
    "def run_rag_completion(\n",
    "    document_dir: str,\n",
    "    query_text: str,\n",
    "    embedding_model: str =\"togethercomputer/m2-bert-80M-8k-retrieval\",\n",
    "    generative_model: str =\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    ) -> str:\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=TogetherLLM(\n",
    "            generative_model,\n",
    "            temperature=0.0,\n",
    "            max_tokens=256,\n",
    "            top_p=0.9,\n",
    "            top_k=35,\n",
    "            repetition_penalty=1.1,\n",
    "            is_chat_model=True,\n",
    "            completion_to_prompt=completion_to_prompt\n",
    "        ),\n",
    "        embed_model=TogetherEmbedding(embedding_model)\n",
    "    )\n",
    "    documents = SimpleDirectoryReader(document_dir).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    response = index.as_query_engine(similarity_top_k=5).query(query_text)\n",
    "\n",
    "    return str(response)\n",
    "\n",
    "\n",
    "query_text = \"What is this document about?\"\n",
    "document_dir = './Documents'\n",
    "\n",
    "response = run_rag_completion(document_dir, query_text)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
